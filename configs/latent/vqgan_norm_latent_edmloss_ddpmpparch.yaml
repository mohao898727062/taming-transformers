#backbone_arch DhariwalUNet
#model:latent pfgmpp
model:
  base_learning_rate: 1.5e-09
  target: other.edm.training.networks.EDMLatent
  params:
    img_resolution: 64
    img_channels: 3
    label_dim: 0
    first_stage_config:
      target: taming.models.vqgan.VQModelInterface
      params:
        ckpt_path: first_stage_model/vqgan_ckpt/last.ckpt
        embed_dim: 3
        n_embed: 1024
        ddconfig:
          double_z: false
          z_channels: 3
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult: [1,1,2]
          num_res_blocks: 2
          attn_resolutions: [16]
          dropout: 0.0
        lossconfig:
          target: taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
          params:
            disc_conditional: False
            disc_in_channels: 3
            disc_start: 10000
            disc_weight: 0.8
            codebook_weight: 1.0
    
    unetconfig: 
      target: other.edm.training.networks.SongUNet   #network architecture SongUnet ddpmpp
      params:
        img_resolution: 64
        in_channels: 3
        out_channels: 3
        label_dim: 0
        embedding_type: positional
        encoder_type: standard
        decoder_type: standard
        channel_mult_noise: 1
        resample_filter: [1,1]
        model_channels: 128
        channel_mult: [1,2,2,2]
     
    lossconfig:
      target: other.edm.training.loss.EDMLoss
      loss_caling: 1
      
               
data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 1
    num_workers: 4
    train:
      target: taming.data.latent.ffhq_latent.FFHQTrain
      params:
        file_dir: "data/latent/ffhq/vqgan_ffhq_latents_train.npy"
      
    validation:
      target: taming.data.latent.ffhq_latent.FFHQValidation
      params:
        file_dir: "data/latent/ffhq/vqgan_ffhq_latents_validation.npy"
   
